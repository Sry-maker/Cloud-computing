# 云计算文档

​        项目报告，介绍云系统及分布式存储和分布式计算的方法，包括云系统的架构、 文件分块方法、文件备份方法、文件的一致性策略、计算任务分配机制等。

小组成员：1953060 韩乃超，1854062 许之博



## 系统架构

### 一、云系统架构

​        与传统的服务器相比，云平台可以将物理资源虚拟化为虚拟机资源池，灵活调用软硬件资源，实现对用户的按需访问。而且在运行过程中根据用户并发量不同，实时迁移虚拟机资源，一方面保证提供高质量服务，另一方面最小化资源成本，提高CPU、内存等利用率。

​        该架构主要分为4层，从底层到上层分别是资源层、虚拟层、中间件层、应用层。

1. 资源层：由服务器集群组成。传统服务器要想提供高质量服务，需要性能特别好的服务器（内存高，CPU快，磁盘空间大等），价格昂贵。而服务器集群可以使用以前性能不太好的服务器，利用分布式处理技术，依然可以提供可靠服务，节省费用。

2. 虚拟层：有了物理机集群后，我们需要在物理机上建立虚拟机。建立虚拟机的目的是为了最小化资源成本（最大化资源利用率）。当某段时间连续有小任务量的应用需要处理时，物理机的内存利用率会很低，所以为最大化资源利用率，可以在物理机上独立开辟几个虚拟机，每台虚拟机相当于一个小型服务器，依然可以处理应用请求。

3. 中间件层：

   云平台的核心层，主要功能为：对虚拟机池资源状态进行监测、预警、优化决策。①资源监测：实时监测当前各台虚拟机CPU、内存等使用情况，当然也监测用户应用请求，以便根据应用规模大小进行决策。②预警：防患于未然，根据当前虚拟机资源使用情况预测下一秒用户请求量，以便做出相应资源调整，防止宕机。③优化决策：预警之后，虚拟机要进行资源调度(迁移或伸缩)，采用何种调度策略，才能保证服务和资源利用率是研究重点。由于该层需要对应用进行响应处理，所以需要在虚拟机上搭建操作系统，文件存储系统，以及服务器，当然最应该有负载均衡系统Nginx，其实现中间件层功能，相当于网络中的路由器不处理数据，只进行数据转发，数据处理交有虚拟机上的tomcat服务器执行。（也相当于hadoop中的Namenode，其他虚拟机相当于datanode）。

4. 应用层：给用户提供可视化界面，应用若为存储：比如百度云会给用户提供交互界面，建立文件夹，进行数据存储，在线播放视频等界面，供用户选择操作。应用若为租用服务器：界面应该有租用的服务器资源状态。

### 二、Hadoop介绍

#### hadoop简介

​        Hadoop是一个由Apache基金会所开发的分布式系统基础架构。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的优势进行高速运算和存储。

​        Hadoop实现了一个分布式文件系统（ Distributed File System），其中一个组件是HDFS（Hadoop Distributed File System）。Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。

#### hadoop的优点：

- 高可靠性：Hadoop按位存储和处理数据的能力值得人们信赖。
- 高扩展性：Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。
- 高效性：Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。
- 高容错性：Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。
- 低成本：与一体机、商用数据仓库以及QlikView、Yonghong Z-Suite等数据集市相比，hadoop是开源的，项目的软件成本因此会大大降低

![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fupload-images.jianshu.io%2Fupload_images%2F14603910-d95009e2b8454ef4.png&refer=http%3A%2F%2Fupload-images.jianshu.io&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1643100651&t=18a17b6fb8d9f0577b4e5964564a34dc)

## 文件管理方法

### **一、HDFS**

​        HDFS（Hadoop Distributed File System，Hadoop分布式文件系统），它是一个高度容错性的系统，适合部署在廉价的机器上。HDFS能提供高吞吐量的数据访问，适合那些有着超大数据集（large data set）的应用程序。

**HDFS的设计特点**：

1、大数据文件：非常适合上T级别的大文件或者多个大数据文件的存储。

2、文件分块存储：HDFS会将一个完整的大文件平均分块存储到不同计算器上，它的意义在于读取文件时可以同时从多个主机取不同区块的文件，多主机读取比单主机读取效率要高得多。

3、流式数据访问：一次写入多次读写，这种模式跟传统文件不同，它不支持动态改变文件内容，而是要求让文件一次写入就不做变化，要变化也只能在文件末添加内容。

4、廉价硬件：HDFS可以应用在普通PC机上，这种机制能够让给一些公司用几十台廉价的计算机就可以撑起一个大数据集群。

5、硬件故障：HDFS认为所有计算机都可能会出问题，为了防止某个主机失效读取不到该主机的块文件，它将同一个文件块副本分配到其它某几个主机上，如果其中一台主机失效，可以迅速找另一块副本取文件。

**HDFS的关键元素**：

![HDFS 架构](https://hadoop.apache.org/docs/r1.0.4/cn/images/hdfsarchitecture.gif)

Block：将一个文件进行分块，本实验中默认是128M。

NameNode：保存整个文件系统的目录信息、文件信息及分块信息，这是由唯一一台主机专门保存，当然这台主机如果出错，NameNode就失效了。在Hadoop2.*开始支持activity-standy模式----如果主NameNode失效，启动备用主机运行NameNode。

Namenode负责这个集群的数据备份和分配，在分配过程中，主要考虑下面两个因素：

- 数据安全：在某个节点发生故障时，不会丢失数据备份；
- 网络传输开销：在备份数据同步过程中，尽量减少网络传输中的带宽开销；

DataNode：分布在廉价的计算机上，用于存储Block块文件。

![](cloud_image\namenode.png)

### 二、文件备份方法

​        在分布式系统中，如果某个节点失效了，hdfs有两种机制解决这个问题。

- **HF**：HDFS Federation 是解决 namenode 内存瓶颈问题的水平横向扩展方案。Federation 意味着在集群中将会有多个 namenode/namespace。这些 namenode 之间是联合的，也就是说，他们之间相互独立且不需要互相协调，各自分工，管理自己的区域。分布式的 datanode 被用作通用的数据块存储存储设备。每个 datanode 要向集群中所有的namenode 注册，且周期性地向所有 namenode 发送心跳和块报告，并执行来自所有 namenode的命令。HDFS Federation 并没有完全解决单点故障问题。

- **HA**：Hadoop high ability。namenode里面有数据的映射表，可以控制所有的datanode。为了防止namenode失效，一般会有两个namenode：namenode active 和 namenode standby。只有active对外提高服务。为了时刻保持active与standby的数据同步，一般会有一个JournalNode为了使得standby可以转换为active。zookeeper机制中会有一个fail over controller分别时时监控active和standby的状态。

​        一般集群规模真的很大的时候，会采用HA+Federation的部署方案。也就是每个联合的 namenodes 都是 HA的。

![](cloud_image/%E5%A4%87%E4%BB%BD3.jpg)

### 三、文件的一致性策略

1. namenode和SecondaryNamenode（周期性的保存namenode上的元数据信息）元数据一致性。

2. namenode和datanode心跳机制保证副本的重新创建，如果DataNode死了，原来保存在这个机器上的副本就的重新再别的机器上创建了。

3. DataNode与DataNode副本创建数据是否一致性（网络传输校验和的问题）

4. 租约机制，就是保证一个文件中只允许被一个用户写入数据，租约是由NameNode发放给客户端。

5. 回滚机制，主要体现在hadoop升级的过程中，如果失败，可以恢复到原来的状态



## 计算任务分配机制

​        Hadoop中的文件在存储时被切分成若干个相同大小的数据块，每个数据块备份后分散存储在集群的数据节点上。在进行数据处理时，Hadoop首先将一个作业切分成若干个Map任务并分配到各个节点上并行执行，每个Map任务处理一个数据块，Reduce任务从各个节点上获取执行结果并产生最终的输出。

​        在执行Map任务时，如果数据恰好在Map任务执行的节点上，则称该Map任务是本地任务，否则称为远程任务。执行一个本地的Map任务比执行一个远程的Map任务代价更小，在Hadoop这种网络带宽比较稀缺的环境中体现得更为明显。

​        当前主要的调度算法可以分为静态调度和动态调度两类。

- 静态调度的调度策略事先确定，不考虑集群运行时的动态变化；
- 而动态调度是调度器动态地获取集群的状态信息，并根据这些信息动态调整调度策略。

​         Hadoop默认的任务分配算法使用贪心算法，当一个节点出现空闲的资源时，尽可能地分配本地任务，如果没有本地任务，则分配远程任务。这种任务分配策略的优点是简单、易于实现，同时也减轻了Jobtracker的负担，但也存在着不利于任务执行的本地化的显著缺点。

​        现有的许多任务分配策略或者面向同构环境，或者没有充分利用集群的全局信息，或者在异构环境下无法兼顾执行效率与算法复杂度。也可以考虑基于当前集群状态，以及上轮任务的执行情况，动态进行任务分配，以期达到最优执行效率。

### MapReduce

​        MapReduce是将一个大作业拆分为多个小作业的框架（大作业和小作业应该本质是一样的，只是规模不同），用户需要做的就是决定拆成多少份，以及定义作业本身。

#### map函数和reduce函数

map函数和reduce函数是交给用户实现的，这两个函数定义了任务本身。

- map函数：接受一个键值对（key-value pair），产生一组中间键值对。MapReduce框架会将map函数产生的中间键值对里键相同的值传递给一个reduce函数。
- reduce函数：接受一个键，以及相关的一组值，将这组值进行合并产生一组规模更小的值（通常只有一个或零个值）。

#### mapreduce工作过程

1. MapReduce库先把user program的输入文件划分为M份（M为用户定义），每一份通常有16MB到64MB；然后使用fork将用户进程拷贝到集群内其它机器上。
2. user program的副本中有一个称为master，其余称为worker，master是负责调度的，为空闲worker分配作业（Map作业或者Reduce作业），worker的数量也是可以由用户指定的。
3. 被分配了Map作业的worker，开始读取对应分片的输入数据，Map作业数量是由M决定的，和split一一对应；Map作业从输入数据中抽取出键值对，每一个键值对都作为参数传递给map函数，map函数产生的中间键值对被缓存在内存中。
4. 缓存的中间键值对会被定期写入本地磁盘，而且被分为R个区，R的大小是由用户定义的，将来每个区会对应一个Reduce作业；这些中间键值对的位置会被通报给master，master负责将信息转发给Reduce worker。
5. master通知分配了Reduce作业的worker它负责的分区在什么位置（肯定不止一个地方，每个Map作业产生的中间键值对都可能映射到所有R个不同分区），当Reduce worker把所有它负责的中间键值对都读过来后，先对它们进行排序，使得相同键的键值对聚集在一起。因为不同的键可能会映射到同一个分区也就是同一个Reduce作业，所以排序是必须的。
6. reduce worker遍历排序后的中间键值对，对于每个唯一的键，都将键与关联的值传递给reduce函数，reduce函数产生的输出会添加到这个分区的输出文件中。
7. 当所有的Map和Reduce作业都完成了，master唤醒正版的user program，MapReduce函数调用返回user program的代码。



## 云系统搭建

​        我们的Hadoop(hadoop 2.10.1)集群采用完全分布式方式，搭建于两台物理机器上的四个CentOS(CentOS Linux release 7.9.2009 (Core))虚拟机，分别配置了Hadoop环境和网络进行连接。其中一个NameNode, 三个DataNode。

| 节点   | 节点类型 | IP            |
| ------ | -------- | ------------- |
| Master | NameNode | 192.168.43.25 |
| Slave1 | DataNode | 192.168.43.26 |
| Slave2 | DataNode | 192.168.43.27 |
| Slave3 | DateNode | 192.168.43.28 |



### **初步方案：服务器配置Hadoop**

服务器环境：阿里云轻量应用服务器2核4G,Centos7.9 64位操作系统

软件环境：Docker 19.03

#### 一、启用虚拟内存

```
#创建虚拟内存磁盘卷
dd if=/dev/zero of=/swap/swapadd bs=1024 count=5242880
#将磁盘卷转为虚拟内存卷
mkswap /swap/swapadd
#启用虚拟内存服务
swapon /swap/swapadd
```

#### 二、拉取镜像

```bash
docker pull registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base
```

#### 三、启动容器

建立hadoop用的内部网络

```bash
docker network create --driver=bridge --subnet=172.19.0.0/16  hadoop
```

建立Master容器，映射端口

```bash
docker run -it --network hadoop -h Master --name Master --hostname Master --ip 172.19.0.2 --add-host Slave1:172.19.0.3 --add-host Slave2:172.19.0.4 --add-host Slave3:172.19.0.5 -p 9870:9870 -p 8088:8088 -p 10000:10000 registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base bash 
```

建立Master容器

```bash
docker run -it --network hadoop -h Slave1 --name Slave1 --hostname Slave1 --ip 172.19.0.3 --add-host Master:172.19.0.2 --add-host Slave2:172.19.0.4 --add-host Slave3:172.19.0.5 registry.cn-hangzhou.aliyuncs.com/hadoop_test/hadoop_base bash 
```

修改worker情况 vim /usr/local/hadoop/etc/hadoop/workers

```bash
Master
Slave1
Slave2
Slave3
```

四台机器，都修改host vim /etc/hosts

```
172.20.0.2	Master
172.20.0.3	Slave1
172.20.0.4	Slave2
172.20.0.5	Slave3
```

#### 四、启动Hadoop

进入master，启动hadoop，先格式化hdfs

```
source /etc/profile
#进入Master容器
docker exec -it Master bash
#进入后格式化hdfs
root@Master:/# hadoop namenode -format
```

启动全部，包含hdfs和yarn

```
root@Master:/usr/local/hadoop/sbin# ./start-all.sh
```



![服务器](cloud_image/%E6%9C%8D%E5%8A%A1%E5%99%A8.png)



由于我们的阿里云服务器为轻量型服务器，2核4G，启用虚拟内存后只能使得hadoop在启动时不会因为内存不足down，但是碍于服务器性能，十分卡顿，无法进行mapreduce的运行，至此服务器方案无法进行，转为本地虚拟机。



### **最终方案：本地虚拟机**

vm产品：VMware® Workstation 16 Pro

vm版本：16.2.1 build-18811642

centos：Centos Linux release 7.9.2009 (Core)

#### 一、网络配置

进入VMware的编辑->虚拟网络编辑器->更改设备,**将VMnet0（或其它类型为桥接模式的网络适配器）桥接的网卡由默认的“自动”改为连上互联网的网卡**

![](cloud_image/vm1.png)



#### 二、设置虚拟机的IP、DNS和主机名

##### 设置IP地址、子网掩码和网关，如下图。

```
vi /etc/sysconfig/network-scripts/ifcfg-ens33 
```

![](cloud_image/net1.png)



##### 设置DNS

```
vi /etc/resolv.conf
```

![](cloud_image/net2.png)



##### 设置主机名

```
vi /etc/sysconfig/network
```

![](cloud_image/net3.png)



```
vi /etc/hostname
```

![](cloud_image/net4.png)



##### 重启网卡

```
service network restart
```



##### 其他虚拟机亦如此

#### 三、SSH无密码登录节点

这个操作是要让Master节点可以无密码SSH登陆到各个Slave节点上。

首先生成Master节点的公匙，在Master节点的终端中执行：

```
cd ~/.ssh               # 如果没有该目录，先执行一次ssh localhost
rm ./id_rsa*            # 删除之前生成的公匙（如果有）
ssh-keygen -t rsa       
```

让Master节点需能无密码SSH本机，在Master节点上执行：

```
cat ./id_rsa.pub >> ./authorized_keys
```

完成后可执行sshMaster验证一下（可能需要输入yes，成功后执行exit返回原来的终端）。接着在Master节点将上公匙传输到Slave1节点：

```
scp ~/.ssh/id_rsa.pub hadoop@Slave1:/root
```

接着在Slave1节点上，将ssh公匙加入授权：

```
mkdir ~/.ssh       # 如果不存在该文件夹需先创建，若已存在则忽略
cat ~/id_rsa.pub >> ~/.ssh/authorized_keys
rm ~/id_rsa.pub    
```

如果有其他Slave节点，也要执行将Master公匙传输到Slave节点、在Slave节点上加入授权这两步。

这样，在Master节点上就可以无密码SSH到各个Slave节点了

```
ssh Slave1
```

#### 四、配置PATH变量

```
 vi ~/.bashrc
```

![](cloud_image/path1.png)

```
source ~/.bashrc
```



#### 五、配置环境

修改/usr/local/hadoop/etc/hadoop中的5个配置文件，更多设置项可点击查看官方说明，这里仅设置了正常启动所必须的设置项：slaves、core-site.xml、hdfs-site.xml、mapred-site.xml、yarn-site.xml。

##### 文件slaves

将作为DataNode的主机名写入该文件，每行一个，让Master节点仅作为NameNode使用。

##### 文件**core-site.xml** 改为下面的配置：

```
<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://Master:9000</value>
        </property>
        <property>
                <name>hadoop.tmp.dir</name>
                <value>file:/usr/local/hadoop/tmp</value>
                <description>Abase for other temporary directories.</description>
        </property>
</configuration>
```

##### 文件**hdfs-site.xml**

```
<configuration>
        <property>
                <name>dfs.namenode.secondary.http-address</name>
                <value>Master:50090</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>file:/usr/local/hadoop/tmp/dfs/name</value>
        </property>
        <property>
                <name>dfs.datanode.data.dir</name>
                <value>file:/usr/local/hadoop/tmp/dfs/data</value>
        </property>
</configuration>
```

##### 文件mapred-site.xml（可能需要先重命名，默认文件名为mapred-site.xml.template）

```
<configuration>
        <property>
                <name>mapreduce.framework.name</name>
                <value>yarn</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.address</name>
                <value>Master:10020</value>
        </property>
        <property>
                <name>mapreduce.jobhistory.webapp.address</name>
                <value>Master:19888</value>
        </property>
</configuration>
```

##### 文件yarn-site.xml

```
<configuration>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>Master</value>
        </property>
        <property>
                <name>yarn.nodemanager.aux-services</name>
                <value>mapreduce_shuffle</value>
        </property>
</configuration>
```

##### 配置好后，将Master上的/usr/local/Hadoop文件夹复制到各个节点上。

```
cd /usr/local
sudo rm -r ./hadoop/tmp     # 删除 Hadoop 临时文件
sudo rm -r ./hadoop/logs/*   # 删除日志文件
tar -zcf ~/hadoop.master.tar.gz ./hadoop   # 先压缩再复制
cd ~
scp ./hadoop.master.tar.gz Slave1:/root
```

##### 在Slave1节点上执行

```
sudo rm -r /usr/local/hadoop    # 删掉旧的（如果存在）
sudo tar -zxf ~/hadoop.master.tar.gz -C /usr/local
sudo chown -R root /usr/local/hadoop
```

同样，如果有其他Slave节点，也要执行将hadoop.master.tar.gz传输到Slave节点、在Slave节点解压文件的操作。

##### 首次启动需要先在Master节点执行NameNode的格式化：

```
hdfs namenode -format       # 首次运行需要执行初始化，之后不需要
```

##### 关闭centos防火墙

```
systemctl stop firewalld.service    # 关闭firewall
systemctl disable firewalld.service # 禁止firewall开机启动
```

##### 启动hadoop

```
start-dfs.sh
start-yarn.sh
mr-jobhistory-daemon.sh start historyserver
```

##### 关闭Hadoop集群

```
stop-yarn.sh
stop-dfs.sh
mr-jobhistory-daemon.sh stop historyserver
```



## MapReduce运行

### 数据预处理

我们首先对数据使用python中的pandas进行预处理

```python
import pandas as pd

attributes = ['day_id', 'calling_nbr', 'called_nbr', 'calling_optr',
              'called_optr','calling_city','called_city','calling_roam_city',
              'called_roam_city', 'start_time', 'end_time', 'raw_dur',
              'call_type', 'calling_cell']
df = pd.read_table('data.txt', header=None,index_col=False,names=attributes)
```

缺失值判断，返回为空，无缺失值。

```
print(df.isnull().sum())  # 判断缺失值
```

​        经观察，通话时长和通话的截至时间“end_time"存在异常，于是做出以下处理。将异常时间进行重新计算，如果重新计算得到的时间小于0，则将该行数据删掉。

```python
def compute_time(start,end):
    s=[int(x) for x in str(start).split(':')]
    e=[int(x) for x in str(end).split(':')]
    t=e[0]*3600+e[1]*60+e[2]-s[0]*3600-s[1]*60-s[2]
    return t
```

```python
def compare_time(start,end):
    if compute_time(start,end)<0: return False
    return True
```

```python
for index,row in df.iterrows():
    if row['raw_dur']==0 and compare_time(row['start_time'],row['end_time']):
        if compute_time(row['start_time'],row['end_time'])<0:
            df.drop([index])
            continue
      df.loc[index,'raw_dur']=compute_time(row['start_time'],row['end_time'])
```

  

### mapreduce代码

#### Job

```
        //1、设置job的基础属性
        Configuration configuration = new Configuration();
        Job job = new Job(configuration);
        job.setJarByClass(xxxx.class);
        job.setJobName("xxxx");

        //2、设置Map与Reudce的类
        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);

        //3、设置map与reduce的输出键值类型
        job.setMapOutputKeyClass(Text.class);
        job.setMapOutputValueClass(Text.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(DoubleWritable.class);

        //4、设置输入输出路径
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));

        //5、提交执行
        System.out.println("start");
        job.waitForCompletion(true);
        long endTime = new Date().getTime();
```

#### Task1

##### map

```
            String line = value.toString();
            String[] srcData = line.split("\t");

            String day_id = srcData[0];
            String calling_nbr = srcData[1];
            String called_nbr = srcData[2];
            String start_time = srcData[9];

            context.write(new Text(calling_nbr), new Text(calling_nbr + ' ' + day_id + ' ' + start_time));
            context.write(new Text(called_nbr), new Text(called_nbr + ' ' + day_id + ' ' + start_time));
```

##### reduce

```
            int allcall = 0;
            for (Text value : values) {
                allcall++;
            }

            double averagecall = allcall / 29.0;
            context.write(key, new DoubleWritable(averagecall));
```

#### Task2

##### map

```
            String line = value.toString();
            String[] srcData = line.split("\t");

            String calling_optr = srcData[3];
            String called_optr=srcData[4];
            String call_type = srcData[12];

            context.write(new Text(call_type + calling_optr), new IntWritable(1));
            context.write(new Text(call_type + called_optr), new IntWritable(1));
```

##### reduce

```
            int sum = 0;
            for (IntWritable value : values) {
                sum = sum + value.get();
            }
            context.write(key, new IntWritable(sum));
```

#### Task3

##### map

```

            String line = value.toString();
            String[] srcData = line.split("\t");
            String calling_nbr = srcData[1];
            String called_nbr = srcData[2];
            String start_time = srcData[9];
            String raw_dur = srcData[11];

            //通话开始时间
            String[] startTime = start_time.split(":");
            int startHour = Integer.parseInt(startTime[0]);
            int startMinute = Integer.parseInt(startTime[1]);
            int startSecond = Integer.parseInt(startTime[2]);

            //设置边界及时间段时间
            int[] boundary = {3, 6, 9, 12, 15, 18, 21, 24};
            int[] timePeriod = {0, 0, 0, 0, 0, 0, 0, 0};

            int period = startHour / 3;
            int remainTime = Integer.parseInt(raw_dur);
            int boundaryremain = 3600 * (boundary[period] - startHour) - 60 * startMinute - startSecond;

            //剩余时间小于剩余边界
            if (remainTime <= boundaryremain) {
                timePeriod[period] = remainTime;
            }
            //剩余时间大于剩余边界
            if (remainTime > boundaryremain) {
                timePeriod[period] = boundaryremain;
                remainTime = remainTime - boundaryremain;
                int addperioid = remainTime / (3600 * 3);
                for (int i = 0; i < addperioid; i++) {
                    timePeriod[(period + addperioid) % 8] = 3600 * 3;
                }
                int addremianTime = remainTime % (3600 * 3);
                if (addremianTime != 0) {
                    timePeriod[(period + addperioid + 1) % 8] = addremianTime;
                }
            }


            String str = timePeriod[0] + "\t" + timePeriod[1] + "\t" + timePeriod[2] + "\t" + timePeriod[3] + "\t" + timePeriod[4] + "\t" + timePeriod[5] + "\t" + timePeriod[6] + "\t" + timePeriod[7];
            context.write(new Text(calling_nbr), new Text(str));
            context.write(new Text(called_nbr), new Text(str));
```

##### reduce

```
            double[] timePeriod = {0, 0, 0, 0, 0, 0, 0, 0};
            StringBuilder outputStr = new StringBuilder();
            for (Text value : values) {
                String line = value.toString();
                String[] srcData = line.split("\t");
                timePeriod[0] = Integer.parseInt(srcData[0]) + timePeriod[0];
                timePeriod[1] = Integer.parseInt(srcData[1]) + timePeriod[1];
                timePeriod[2] = Integer.parseInt(srcData[2]) + timePeriod[2];
                timePeriod[3] = Integer.parseInt(srcData[3]) + timePeriod[3];
                timePeriod[4] = Integer.parseInt(srcData[4]) + timePeriod[4];
                timePeriod[5] = Integer.parseInt(srcData[5]) + timePeriod[5];
                timePeriod[6] = Integer.parseInt(srcData[6]) + timePeriod[6];
                timePeriod[7] = Integer.parseInt(srcData[7]) + timePeriod[7];
            }


            double rate = 0;
            for (int i = 0; i < timePeriod.length; i++) {
                rate = timePeriod[i] / Arrays.stream(timePeriod).sum();
                outputStr.append(rate);
                if (i != timePeriod.length - 1) {
                    outputStr.append(" ");
                }
            }

            context.write(key, new Text(outputStr.toString()));
```



#### 程序打包

```
MapReduce-code-1.0-SNAPSHOT.jar
```

#### 上传Master，程序运行

```
hadoop jar MapReduce-code-1.0-SNAPSHOT.jar hadoop.Task1.AverageCommunicationFrequency  /input /output1
hadoop jar MapReduce-code-1.0-SNAPSHOT.jar hadoop.Task2.CommunicationTypePercent  /input /output2
hadoop jar MapReduce-code-1.0-SNAPSHOT.jar hadoop.Task3.CommunicationTimePercent  /input /output3
```



#### Task1**运行结果**

![](cloud_image/task1.png)



#### Task2**运行结果**

![](cloud_image/task2.png)

![](cloud_image\4.png)



![](cloud_image\5.png)

![](cloud_image\6.png)

#### Task3**运行结果**

![](cloud_image/task3.png)



## 性能测试优化



